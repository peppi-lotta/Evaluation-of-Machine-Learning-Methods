{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Nested cross-validation exercise\n",
    "## Nested cross-validation for feature selection with nearest neighbors <br>\n",
    "- Use Python 3 to program both a hyper-parameter selection method based on leave-one-out cross-validation and a nested leave-one-out cross-validation for estimating the prediction performance of models inferred with this automatic selection approach. Use base learning algorithm provided in the exercise, namely the \"use_ith_feature\" method, so that the value of the hyper-parameter i is automatically selected from the range from 1 to 100 of alternative values. The provided base learning algorithm \"use_ith_feature\" is 1-nearest neighbor that uses only the ith feature of the data given to it. The LOOCV based hyper-parameter selection procedure is supposed to select the best feature, e.g. the value of i, based on C-index evaluated with predictions obtained with leave-one-out cross-validation. A ready-made implementation of C-index is also provided in the exercise. In nested leave-one-out cross-validation, a C_index value is further evaluated on the predictions obtained from an outer leave-one-out cross-validation. During each round of this outer LOOCV, the whole feature selection process based on inner LOOCV is separately done and the selected feature is used for prediction for the test data point held out during that round of the outer LOOCV. Accordingly, the actual learning algorithm, whose prediction performance will be evaluated with nested CV, is the one that automatically selects the single best feature with leave-one-out cross-validation based model selection (see the lectures and the pseudo codes presented on them for more info on this interpretation).\n",
    "- Note that since the hold-out set in LOOCV has only a single datum but C-index requires at least two data points. The solution in this exercise is to \"pool\" the predictions of all LOOCV rounds of a single LOOCV computation into an array of length of the data used in that LOOCV computation and then compute C-index on that array and the corresponding true outputs. This pooling approach, however, does have its weaknesses, since C-index computed from pooled LOOCV outputs may sometimes be a heavily biased estimator of the true C-index. This has been considered in detail in our previous research (and other group's too as seen in the references) that is available here:\n",
    "http://dx.doi.org/10.1177/0962280218795190\n",
    "where AUC, a special case of C-index, is considered. The study goes quite deep into the problem of AUC estimation with CV, and you can read it if you are interested about the research carried out in our laboratory, while EMLM course does not go that far and this year's exercise unfortunately still has this non-optimal pooling approach in use.\n",
    "- Compare the C-index produced by nested leave-one-out CV with the result of ordinary leave-one-out CV with the best value of i e.g. the feature providing the highest LOOCV C-index, and show the C-index difference between the two methods.\n",
    "- Use the provided implementation of the \"use_ith_feature\" learning algorithm and C-index functions in your exercise.\n",
    "\n",
    "As a summary, for completing this exercise implement the following steps: \n",
    "_______________________________________________________________\n",
    "#### 1. Use leave-one-out cross-validation for determining the optimal i-parameter for the data (X_alternative.csv, y_alternative.csv) from the set of possible values of i e.g. {1,...,100}. When you have found the optimal i, save the corresponding C-index (call it loo_c_index) for this parameter.\n",
    "#### 2. Similarly, use nested leave-one-out cross-validation (leave-one-out both in outer and inner folds) for estimating the C-index (call it nloo_c_index) of the method that selects the best feature with leave-one-out approach. \n",
    "#### 3. Return both this notebook and as a PDF-file made from it in the exercise submit page. \n",
    "_______________________________________________________________\n",
    "\n",
    "Remember to use the provided learning algorithm use_ith_feature and C-index functions in your implementation! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this cell import all libraries you need. For example: \n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provided functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "C-index function: \n",
    "- INPUTS: \n",
    "'y' an array of the true output values\n",
    "'yp' an array of predicted output values\n",
    "- OUTPUT: \n",
    "The c-index value\n",
    "\"\"\"\n",
    "def cindex(y, yp):\n",
    "    n = 0\n",
    "    h_num = 0 \n",
    "    for i in range(0, len(y)):\n",
    "        t = y[i]\n",
    "        p = yp[i]\n",
    "        for j in range(i+1, len(y)):\n",
    "            nt = y[j]\n",
    "            np = yp[j]\n",
    "            if (t != nt): \n",
    "                n = n + 1\n",
    "                if (p < np and t < nt) or (p > np and t > nt): \n",
    "                    h_num += 1\n",
    "                elif (p == np):\n",
    "                    h_num += 0.5\n",
    "    return h_num/n\n",
    "\n",
    "\"\"\"\n",
    "Self-contained 1-nearest neighbor using only a single feature\n",
    "- INPUTS: \n",
    "'X_train' a numpy matrix of the X-features of the train data points\n",
    "'y_train' a numpy matrix of the output values of the train data points\n",
    "'X_test' a numpy matrix of the X-features of the test data points\n",
    "'i' the index of the feature to be used with 1-nearest neighbor\n",
    "- OUTPUT: \n",
    "'y_predictions' a list of the output value predictions\n",
    "\"\"\"\n",
    "def use_ith_feature(X_train, y_train, X_test, i):\n",
    "    y_predictions = []\n",
    "    for test_ind in range(0, X_test.shape[0]):\n",
    "        diff = X_test[test_ind, i] - X_train[:, i]\n",
    "        distances = np.sqrt(diff * diff)\n",
    "        sort_inds = np.array(np.argsort(distances), dtype=int)\n",
    "        y_predictions.append(y_train[sort_inds[0]])\n",
    "    return y_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.619350</td>\n",
       "      <td>0.129018</td>\n",
       "      <td>0.131995</td>\n",
       "      <td>0.081591</td>\n",
       "      <td>0.536646</td>\n",
       "      <td>0.856475</td>\n",
       "      <td>0.269039</td>\n",
       "      <td>0.542290</td>\n",
       "      <td>0.953448</td>\n",
       "      <td>0.394755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.684800</td>\n",
       "      <td>0.240149</td>\n",
       "      <td>0.534512</td>\n",
       "      <td>0.743666</td>\n",
       "      <td>0.613363</td>\n",
       "      <td>0.044856</td>\n",
       "      <td>0.252338</td>\n",
       "      <td>0.481110</td>\n",
       "      <td>0.554211</td>\n",
       "      <td>0.624228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.894394</td>\n",
       "      <td>0.420882</td>\n",
       "      <td>0.409956</td>\n",
       "      <td>0.546907</td>\n",
       "      <td>0.716692</td>\n",
       "      <td>0.403165</td>\n",
       "      <td>0.510235</td>\n",
       "      <td>0.487369</td>\n",
       "      <td>0.016227</td>\n",
       "      <td>0.082877</td>\n",
       "      <td>...</td>\n",
       "      <td>0.580510</td>\n",
       "      <td>0.136212</td>\n",
       "      <td>0.198161</td>\n",
       "      <td>0.020115</td>\n",
       "      <td>0.800232</td>\n",
       "      <td>0.116786</td>\n",
       "      <td>0.043594</td>\n",
       "      <td>0.170237</td>\n",
       "      <td>0.310986</td>\n",
       "      <td>0.530988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.523794</td>\n",
       "      <td>0.405804</td>\n",
       "      <td>0.522332</td>\n",
       "      <td>0.575482</td>\n",
       "      <td>0.918109</td>\n",
       "      <td>0.769205</td>\n",
       "      <td>0.984211</td>\n",
       "      <td>0.347833</td>\n",
       "      <td>0.441513</td>\n",
       "      <td>0.452526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179802</td>\n",
       "      <td>0.446898</td>\n",
       "      <td>0.039641</td>\n",
       "      <td>0.245948</td>\n",
       "      <td>0.310818</td>\n",
       "      <td>0.045215</td>\n",
       "      <td>0.832020</td>\n",
       "      <td>0.401422</td>\n",
       "      <td>0.410235</td>\n",
       "      <td>0.807586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.385701</td>\n",
       "      <td>0.403238</td>\n",
       "      <td>0.712307</td>\n",
       "      <td>0.709464</td>\n",
       "      <td>0.193943</td>\n",
       "      <td>0.851972</td>\n",
       "      <td>0.469135</td>\n",
       "      <td>0.059363</td>\n",
       "      <td>0.231504</td>\n",
       "      <td>0.755357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.686894</td>\n",
       "      <td>0.937015</td>\n",
       "      <td>0.564522</td>\n",
       "      <td>0.193605</td>\n",
       "      <td>0.019359</td>\n",
       "      <td>0.075442</td>\n",
       "      <td>0.335750</td>\n",
       "      <td>0.319551</td>\n",
       "      <td>0.430338</td>\n",
       "      <td>0.108209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.409570</td>\n",
       "      <td>0.318821</td>\n",
       "      <td>0.486487</td>\n",
       "      <td>0.785272</td>\n",
       "      <td>0.398906</td>\n",
       "      <td>0.520888</td>\n",
       "      <td>0.937629</td>\n",
       "      <td>0.588577</td>\n",
       "      <td>0.572203</td>\n",
       "      <td>0.245983</td>\n",
       "      <td>...</td>\n",
       "      <td>0.526579</td>\n",
       "      <td>0.733458</td>\n",
       "      <td>0.512536</td>\n",
       "      <td>0.764717</td>\n",
       "      <td>0.966871</td>\n",
       "      <td>0.169473</td>\n",
       "      <td>0.837364</td>\n",
       "      <td>0.594323</td>\n",
       "      <td>0.602016</td>\n",
       "      <td>0.353801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.949962</td>\n",
       "      <td>0.564682</td>\n",
       "      <td>0.553774</td>\n",
       "      <td>0.758504</td>\n",
       "      <td>0.988201</td>\n",
       "      <td>0.099114</td>\n",
       "      <td>0.244342</td>\n",
       "      <td>0.577558</td>\n",
       "      <td>0.414522</td>\n",
       "      <td>0.562557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940847</td>\n",
       "      <td>0.857497</td>\n",
       "      <td>0.814041</td>\n",
       "      <td>0.525641</td>\n",
       "      <td>0.349517</td>\n",
       "      <td>0.893160</td>\n",
       "      <td>0.888424</td>\n",
       "      <td>0.071549</td>\n",
       "      <td>0.321960</td>\n",
       "      <td>0.495570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.051626</td>\n",
       "      <td>0.989836</td>\n",
       "      <td>0.197861</td>\n",
       "      <td>0.212656</td>\n",
       "      <td>0.283483</td>\n",
       "      <td>0.701698</td>\n",
       "      <td>0.248090</td>\n",
       "      <td>0.028091</td>\n",
       "      <td>0.459789</td>\n",
       "      <td>0.810378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.274311</td>\n",
       "      <td>0.482525</td>\n",
       "      <td>0.956788</td>\n",
       "      <td>0.665012</td>\n",
       "      <td>0.573106</td>\n",
       "      <td>0.913248</td>\n",
       "      <td>0.917017</td>\n",
       "      <td>0.431100</td>\n",
       "      <td>0.205266</td>\n",
       "      <td>0.764825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.494949</td>\n",
       "      <td>0.965460</td>\n",
       "      <td>0.077357</td>\n",
       "      <td>0.804820</td>\n",
       "      <td>0.744440</td>\n",
       "      <td>0.097722</td>\n",
       "      <td>0.973724</td>\n",
       "      <td>0.271387</td>\n",
       "      <td>0.168571</td>\n",
       "      <td>0.660916</td>\n",
       "      <td>...</td>\n",
       "      <td>0.659134</td>\n",
       "      <td>0.704446</td>\n",
       "      <td>0.657918</td>\n",
       "      <td>0.069332</td>\n",
       "      <td>0.224993</td>\n",
       "      <td>0.318439</td>\n",
       "      <td>0.624500</td>\n",
       "      <td>0.270147</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.877994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.416414</td>\n",
       "      <td>0.749550</td>\n",
       "      <td>0.016394</td>\n",
       "      <td>0.296416</td>\n",
       "      <td>0.889423</td>\n",
       "      <td>0.607366</td>\n",
       "      <td>0.928859</td>\n",
       "      <td>0.563848</td>\n",
       "      <td>0.383554</td>\n",
       "      <td>0.555825</td>\n",
       "      <td>...</td>\n",
       "      <td>0.445784</td>\n",
       "      <td>0.867884</td>\n",
       "      <td>0.332692</td>\n",
       "      <td>0.934119</td>\n",
       "      <td>0.594126</td>\n",
       "      <td>0.642161</td>\n",
       "      <td>0.999535</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>0.231834</td>\n",
       "      <td>0.687030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.780416</td>\n",
       "      <td>0.127139</td>\n",
       "      <td>0.018303</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>0.413304</td>\n",
       "      <td>0.354374</td>\n",
       "      <td>0.284472</td>\n",
       "      <td>0.744184</td>\n",
       "      <td>0.860962</td>\n",
       "      <td>0.539105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.510462</td>\n",
       "      <td>0.721482</td>\n",
       "      <td>0.988515</td>\n",
       "      <td>0.862916</td>\n",
       "      <td>0.489859</td>\n",
       "      <td>0.343975</td>\n",
       "      <td>0.488433</td>\n",
       "      <td>0.174983</td>\n",
       "      <td>0.482026</td>\n",
       "      <td>0.103623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.619350  0.129018  0.131995  0.081591  0.536646  0.856475  0.269039   \n",
       "1   0.894394  0.420882  0.409956  0.546907  0.716692  0.403165  0.510235   \n",
       "2   0.523794  0.405804  0.522332  0.575482  0.918109  0.769205  0.984211   \n",
       "3   0.385701  0.403238  0.712307  0.709464  0.193943  0.851972  0.469135   \n",
       "4   0.409570  0.318821  0.486487  0.785272  0.398906  0.520888  0.937629   \n",
       "..       ...       ...       ...       ...       ...       ...       ...   \n",
       "95  0.949962  0.564682  0.553774  0.758504  0.988201  0.099114  0.244342   \n",
       "96  0.051626  0.989836  0.197861  0.212656  0.283483  0.701698  0.248090   \n",
       "97  0.494949  0.965460  0.077357  0.804820  0.744440  0.097722  0.973724   \n",
       "98  0.416414  0.749550  0.016394  0.296416  0.889423  0.607366  0.928859   \n",
       "99  0.780416  0.127139  0.018303  0.147824  0.413304  0.354374  0.284472   \n",
       "\n",
       "          7         8         9   ...        20        21        22        23  \\\n",
       "0   0.542290  0.953448  0.394755  ...  0.684800  0.240149  0.534512  0.743666   \n",
       "1   0.487369  0.016227  0.082877  ...  0.580510  0.136212  0.198161  0.020115   \n",
       "2   0.347833  0.441513  0.452526  ...  0.179802  0.446898  0.039641  0.245948   \n",
       "3   0.059363  0.231504  0.755357  ...  0.686894  0.937015  0.564522  0.193605   \n",
       "4   0.588577  0.572203  0.245983  ...  0.526579  0.733458  0.512536  0.764717   \n",
       "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
       "95  0.577558  0.414522  0.562557  ...  0.940847  0.857497  0.814041  0.525641   \n",
       "96  0.028091  0.459789  0.810378  ...  0.274311  0.482525  0.956788  0.665012   \n",
       "97  0.271387  0.168571  0.660916  ...  0.659134  0.704446  0.657918  0.069332   \n",
       "98  0.563848  0.383554  0.555825  ...  0.445784  0.867884  0.332692  0.934119   \n",
       "99  0.744184  0.860962  0.539105  ...  0.510462  0.721482  0.988515  0.862916   \n",
       "\n",
       "          24        25        26        27        28        29  \n",
       "0   0.613363  0.044856  0.252338  0.481110  0.554211  0.624228  \n",
       "1   0.800232  0.116786  0.043594  0.170237  0.310986  0.530988  \n",
       "2   0.310818  0.045215  0.832020  0.401422  0.410235  0.807586  \n",
       "3   0.019359  0.075442  0.335750  0.319551  0.430338  0.108209  \n",
       "4   0.966871  0.169473  0.837364  0.594323  0.602016  0.353801  \n",
       "..       ...       ...       ...       ...       ...       ...  \n",
       "95  0.349517  0.893160  0.888424  0.071549  0.321960  0.495570  \n",
       "96  0.573106  0.913248  0.917017  0.431100  0.205266  0.764825  \n",
       "97  0.224993  0.318439  0.624500  0.270147  0.289065  0.877994  \n",
       "98  0.594126  0.642161  0.999535  0.473911  0.231834  0.687030  \n",
       "99  0.489859  0.343975  0.488433  0.174983  0.482026  0.103623  \n",
       "\n",
       "[100 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.363947</td>\n",
       "      <td>0.654349</td>\n",
       "      <td>0.370083</td>\n",
       "      <td>0.833793</td>\n",
       "      <td>0.198503</td>\n",
       "      <td>0.198334</td>\n",
       "      <td>0.308125</td>\n",
       "      <td>0.411723</td>\n",
       "      <td>0.724286</td>\n",
       "      <td>0.115418</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132256</td>\n",
       "      <td>0.913207</td>\n",
       "      <td>0.485352</td>\n",
       "      <td>0.448907</td>\n",
       "      <td>0.917852</td>\n",
       "      <td>0.074258</td>\n",
       "      <td>0.073575</td>\n",
       "      <td>0.141819</td>\n",
       "      <td>0.572679</td>\n",
       "      <td>0.900572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.363947  0.654349  0.370083  0.833793  0.198503  0.198334  0.308125   \n",
       "\n",
       "         7         8         9   ...        20        21        22        23  \\\n",
       "0  0.411723  0.724286  0.115418  ...  0.132256  0.913207  0.485352  0.448907   \n",
       "\n",
       "         24        25        26        27        28        29  \n",
       "0  0.917852  0.074258  0.073575  0.141819  0.572679  0.900572  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of missing values in dataFrame x:  0\n",
      "Count of missing values in dataFrame y:  0\n"
     ]
    }
   ],
   "source": [
    "#load data \n",
    "x_df = pd.read_csv('X_alternative.csv', header=None).T\n",
    "display(x_df)\n",
    "\n",
    "y_df = pd.read_csv('Y_alternative.csv', header=None).T\n",
    "display(y_df)\n",
    "\n",
    "#Check for missing values\n",
    "print('Count of missing values in dataFrame x: ', x_df.isna().sum().sum())\n",
    "print('Count of missing values in dataFrame y: ', y_df.isna().sum().sum())\n",
    "\n",
    "#Seems the data is all in the same scale and no values seem to be missing so the data doesn't need any action "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make test/train splits\n",
    "def split(x_df, y_df, row): \n",
    "\n",
    "        #separate a row from x_df to use as test set\n",
    "        xt = x_df[row]\n",
    "        x_test = np.array([xt])\n",
    "\n",
    "        #drop the testing row from the training set\n",
    "        xtr = x_df.drop([row], axis=1)\n",
    "        x_train = np.array(xtr).T\n",
    "\n",
    "        #drop testing rows label from trainin labels\n",
    "        yt = y_df.drop([row], axis=1)\n",
    "        y_train = np.array(yt).T\n",
    "        \n",
    "        return x_test, x_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave-one-out implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loop trough x_df rows. Use the selected row for testing and rest of the rows for training\n",
    "#Goal: choose i \n",
    "\n",
    "#array to store cindexes \n",
    "cindex_df = pd.DataFrame(columns=['c_index'], index=range(0,x_df.shape[0]))\n",
    "\n",
    "#testing labels for cindex\n",
    "y_test = np.array(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#loop through the attributes \n",
    "for i in range(0, x_df.shape[0]):\n",
    "    \n",
    "    #loop x_df rows\n",
    "    y_predictions = []\n",
    "    for c in range(0, x_df.shape[1]):\n",
    "\n",
    "        x_test, x_train, y_train = split(x_df, y_df, c)\n",
    "    \n",
    "        #make predictions based on different attributes\n",
    "        #make prediction\n",
    "        y_pred = use_ith_feature(x_train, y_train, x_test, i)\n",
    "        #get cindex for this attribute\n",
    "        y_predictions = np.append(y_predictions, y_pred[0][0])\n",
    "    \n",
    "    #initialize arrays for cindex funtion\n",
    "    y = y_test[0]\n",
    "    yp = y_predictions\n",
    "    \n",
    "    #add cindexis to dataframe\n",
    "    cind = cindex(y, yp)\n",
    "    cindex_df.at[i,'c_index'] = cind\n",
    "\n",
    "#printing something to indicate we are ready.\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.37931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.532184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.356322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.585057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.55977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.506897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.47931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.597701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.385057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.487356</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     c_index\n",
       "0    0.37931\n",
       "1   0.532184\n",
       "2   0.356322\n",
       "3   0.585057\n",
       "4    0.55977\n",
       "..       ...\n",
       "95  0.506897\n",
       "96   0.47931\n",
       "97  0.597701\n",
       "98  0.385057\n",
       "99  0.487356\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best attribute for regression is \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>0.662069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     c_index\n",
       "76  0.662069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display all indexis\n",
    "display(cindex_df)\n",
    "\n",
    "best = cindex_df[cindex_df.c_index.max() == cindex_df['c_index']]\n",
    "\n",
    "#attribute with maximum value \n",
    "print('The best attribute for regression is ')\n",
    "display(best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested-leave-one-out implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new column names\n",
    "names = []\n",
    "for i in range(0, x_train_outer.shape[0]):\n",
    "    names = np.append(names, int(i))\n",
    "\n",
    "cindex_df_inner = pd.DataFrame(columns=['c_index'], index=range(0,x_df.shape[0])) #initialize array to store inner loops cindex values\n",
    "\n",
    "y_test_outer = np.array(y_df)\n",
    "y_predictions_outer = []\n",
    "\n",
    "cindex_df_outer = pd.DataFrame(columns=['c_index'], index=range(0,x_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer split:  0\n",
      "Outer split:  1\n",
      "Outer split:  2\n",
      "Outer split:  3\n",
      "Outer split:  4\n",
      "Outer split:  5\n",
      "Outer split:  6\n",
      "Outer split:  7\n",
      "Outer split:  8\n",
      "Outer split:  9\n",
      "Outer split:  10\n",
      "Outer split:  11\n",
      "Outer split:  12\n",
      "Outer split:  13\n",
      "Outer split:  14\n",
      "Outer split:  15\n",
      "Outer split:  16\n",
      "Outer split:  17\n",
      "Outer split:  18\n",
      "Outer split:  19\n",
      "Outer split:  20\n",
      "Outer split:  21\n",
      "Outer split:  22\n",
      "Outer split:  23\n",
      "Outer split:  24\n",
      "Outer split:  25\n",
      "Outer split:  26\n",
      "Outer split:  27\n",
      "Outer split:  28\n",
      "Outer split:  29\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "#outer split \n",
    "for c in range(0, x_df.shape[1]):\n",
    "    print('Outer split: ', c)\n",
    "    \n",
    "    #call split funtion\n",
    "    x_test_outer, x_train_outer, y_train_outer = split(x_df, y_df, c)\n",
    "    \n",
    "    #dataframes for inner split \n",
    "    x_df_inner = pd.DataFrame((x_train_outer.T))\n",
    "    x_df_inner.columns = names\n",
    "    y_df_inner = pd.DataFrame((y_train_outer.T))\n",
    "    y_df_inner.columns = names\n",
    "    \n",
    "    y_test_inner = np.array(y_df_inner) \n",
    "    \n",
    "    #loop through the attributes \n",
    "    for i in range(0, x_df.shape[0]):\n",
    "        \n",
    "        y_predictions_inner = []\n",
    "        \n",
    "        #inner split\n",
    "        for d in range(0, x_df_inner.shape[1]):\n",
    "            x_test_inner, x_train_inner, y_train_inner = split(x_df_inner, y_df_inner, d)\n",
    "            \n",
    "            #make predictions based on different attributes\n",
    "            y_pred = use_ith_feature(x_train_inner, y_train_inner, x_test_inner, i)\n",
    "            #get cindex for this attribute\n",
    "            y_predictions_inner = np.append(y_predictions_inner, y_pred[0][0])\n",
    "        \n",
    "        #initialize arrays for cindex funtion\n",
    "        y = y_test_inner[0]\n",
    "        yp = y_predictions_inner\n",
    "\n",
    "        #add cindexis to dataframe\n",
    "        cind = cindex(y, yp)\n",
    "        cindex_df_inner.at[i,'c_index'] = cind\n",
    "    \n",
    "    #attribute with maximum value \n",
    "    best = np.argmax(cindex_df_inner.values)\n",
    "    \n",
    "    #make predictions based on different attributes\n",
    "    y_pred = use_ith_feature(x_train_outer, y_train_outer, x_test_outer, best)\n",
    "    \n",
    "    #get cindex for this attribute\n",
    "    y_predictions_outer = np.append(y_predictions_outer, y_pred[0][0])\n",
    "    \n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C-index of the model  is  0.5149425287356322\n"
     ]
    }
   ],
   "source": [
    "#initialize arrays for cindex funtion\n",
    "y = y_test_outer[0]\n",
    "yp = y_predictions_outer\n",
    "\n",
    "#add cindexis to dataframe\n",
    "cind = cindex(y, yp)\n",
    "\n",
    "#The actual c-index of the model \n",
    "print('C-index of the model  is ', cind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "This proved the fact that evaluation done at the same time as hyperparameter gives over optimistic approximation.\n",
    "<strong>0.66</strong> was the c-index value of the simple cross-validation. Nested-cross-validation gave us a value of <strong>0.51</strong>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
